IEEE SYMPOSIUM ON SECURITY AND PRIVACY

Privacy Engineering: Shaping an Emerging Field of Research and Practice

Seda Gürses | Princeton University Jose M. del Alamo | Universidad Politécnica de Madrid

e emerging ﬁeld of privacy engineering responds to the gap between research and practice, systematizing and evaluating approaches to capture and address privacy issues while engineering information systems.

P rivacy engineering is an emerging research framework that focuses on designing, implementing, adapting, and evaluating theories, methods, techniques, and tools to systematically capture and address privacy issues in the development of sociotechnical systems.
We primarily situate the eld in so ware engineering yet expect it to build on an intradisciplinary foundation, leveraging techniques and tools from various computer science subdisciplines, such as security engineering, human–computer interaction, and machine learning.Because law, societal norms, ethical conceptualizations, and technological advances inform privacy, the eld is also inevitably interdisciplinary. Furthermore, developing a robust practice will bene t from knowledge of existing business practices as well as organizational studies and psychology. Finally, we expect legislative, policy, and organizational schemes to play a role in incentivizing the development and adoption of privacy engineering in practice;1 these also require evaluation through an engineering-centric lens.
A ention to privacy engineering as a research topic increased dramatically a er 2012 (see Figure 1). To

facilitate the development of this emerging eld, we organized the First International Workshop on Privacy Engineering (IWPE), co-located with 36th IEEE Symposium on Security and Privacy. IWPE provides a forum for those interested in tackling the gaps and challenges in privacy engineering. With its explicit focus on engineering techniques and its interdisciplinary program commi ee with members from computer science, law, policy, social sciences, humanities, and design, the workshop complements existing venues that focus mainly on presenting privacy solutions, like the Symposium on Usable Privacy and Security (h ps://cups.cs.cmu.edu /soups), or that treat privacy as a sub eld of security engineering, like the Privacy Enhancing Technologies Symposium (h ps://petsymposium.org).
e rst iteration of the workshop a racted 47 delegates from academia, industry, government, and civil society. e presentations introduced di erent models and frameworks for understanding privacy; illustrated several methods, techniques, and tools; and provided case studies of privacy-engineering practice in enterprise systems. e programs and presentations can be found at the workshop website, h p://ieee-security .org/TC/SPW2015/IWPE.

40

March/April 2016

Copublished by the IEEE Computer and Reliability Societies

1540-7993/16/$33.00 © 2016 IEEE

The Need for Privacy Engineering
Privacy research in computer science has produced a rich array of privacy solutions; however, the integration of these into everyday engineering practice has been slow. In recent years, reports of privacy violations and technology companies’ failure to fulfill basic data protection requirements have become commonplace, suggesting that we’re far from applying privacy design know-how in practice. The consequences are most evident in the exorbitant number of data breaches: in the US alone, 4,700 breaches have been made public since 2005.2
But when it comes to privacy, a data breach is only one concern among many. Subtle engineering decisions that ignore users’ privacy needs might have far-­reaching consequences. Recent highlights include Snapchat violating user expectations and privacy by not deleting users’ messages, Firefox extension NoScript’s defaults leading to deanonymization attacks on Tor users, and Facebook apps allowing the sharing of users’ friend networks with advertisers. Moreover, when these design decisions concern global infrastructures, such as cloud services, grids, and mobile networks, privacy protections applied at higher layers might be rendered moot. Past reports of Apple, Google, and Microsoft collecting location information gathered by their respective mobile devices from Wi-Fi hotspots—even when users turn off location tracking—and Snowden’s revelations about the US National Security Agency and the UK Government Communications Headquarters surveillance programs illustrate such domino effects.3
The different examples underscore that addressing privacy is relevant when engineering technical infrastructures, implementing organizational controls, and designing user experience (UX). They also imply that privacy solutions are potentially unknown to engineering teams, not practical to integrate into engineering activities, not of interest to the organizations, or non­ existent. Which of these cases hold and when? And what would it take to facilitate an engineering practice that addresses these issues? These remain open questions.
Researchers and practitioners who engage in the topic have made groundbreaking contributions to the field but have rarely attended to the development of a privacy-engineering practice. Their contributions include a wide array of technical solutions that help protect users’ privacy from diverse adversaries and in different social contexts.4 These solutions are informed by rigorous investigations into particular ways in which new technologies threaten privacy.5,6 They also illustrate the way experts successfully utilize or transform techniques from various computer science subfields, such as information security, software engineering, and social computing.7 However, few of these efforts are invested

No. of articles

120
100
80
60
40
20
0 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 Year
Figure 1. The growing number of published privacy-engineering articles.
in systematizing or generalizing their approaches so other organizations and engineers can adopt and integrate them into their daily practices. In the research community, some have proposed monolithic privacyengineering methods; 8,9 however, these tend to assume a one-size-fits-all approach that disregards context, such as organization type and development practices, imminent privacy threats, and informational norms.
Motivating a New Field of Study
Privacy engineering addresses the lack of generalization in existing approaches; shortage in efforts to integrate different subdisciplines’ techniques and tools; the need to evaluate proposed approaches in different social, organizational, technical, and legal contexts; and concrete challenges emerging from the evolution of engineering practices, technical architectures, legal frameworks, and social expectations. Included in this research framework are projects that critically assess ways to respond to regulators’ and organizations’ increasing demands to implement and settle policy “through architecture, configuration, interfaces, and default settings,”10 also called privacy or data protection by design.11,12 The field intends to address these gaps by consolidating existing privacy research.
Over the past several decades, computer scientists have recognized the quest to build privacy-friendly systems as a research challenge. Most efforts have followed three prominent approaches. The first is what Sarah Spiekermann and Lorrie F. Cranor identified as privacy by architecture,8 which aims to minimize the collection or inference of sensitive information by un­intended parties, typically service providers. Researchers develop technologies that enhance privacy by applying techniques that hard-code constraints on data collection and processing in systems and by ensuring that no entity can single-handedly undo these constraints.

www.computer.org/security

41

IEEE SYMPOSIUM ON SECURITY AND PRIVACY

Privacy-enhancing technologies (PETs), such as Tor approach a empts to answer ambitious questions such

for anonymous communications or private informa- as whether we can develop mechanisms that use machine

tion retrieval protocols for con dential search, are learning to reveal discrimination and social sorting, and

developed using this approach. PETs can be used as what properties constitute a fair sociotechnical system.15

stand-alone privacy technologies, like Tor, or func- All three approaches fundamentally di er in what

tion as the primitives in a privacy engineer’s toolbox, they consider to be privacy problems and solutions.

as in the case of zero-knowledge proofs and di eren- is might be seen as productive plurality in research.

tial privacy.

However, isolation between the research communities;

Spiekermann and Cranor identi ed a second their varying positions on the role of technology, law,

approach as privacy by policy.8 It aims at “protecting and society; and the distance between researchers and

consumer data from accidental disclosure or misuses practitioners lead to gaps and vulnerabilities.

and facilitating informed choice options” in other In practice, privacy-by-policy activities are o en

words, enforcing measures

limited to privacy pol-

to ensure compli-

icy statements and

ance with principles of data protection

Purely technical approaches might prove

checkboxes for consent and don’t result

laws in information

insu cient for aligning nuanced legal

in changes to engi-

systems. Depending on jurisdiction, these

policies with engineering artifacts.

neering practice or system design. ese

requirements might

activities have mainly

include specifying

been the bailiwick

and notifying users of the

of the legal team,10 mem-

purpose of collection; limiting collection and use bers of which might not have an in-depth understand-

to this purpose; being transparent about additional ing of engineering privacy mechanisms’ potential and

recipients of the data; and providing users access to limitations. Purely technical approaches might prove

their data for veri cation, correction, and deletion. insu cient for aligning nuanced legal policies with

Proposed technologies include policy speci cation engineering artifacts and can fall short of addressing

languages, policy negotiation and enforcement mech- responsibilities across organizations. In the absence of

anisms, and design techniques to improve the readabil- normative guiding principles and evaluation, privacy-

ity of privacy policies.

by-policy approaches might result in a set of procedures

A third approach, let’s call it privacy by interaction, that ful ll compliance requirements but provide li le

focuses on sociotechnical designs that would improve e ective protection. Moreover, top-down decisions to

users’ agency with respect to privacy in social se ings. introduce data protection mechanisms, if insensitive

e approach captures privacy ma ers that arise, for to the organization’s engineering culture, might be met

example, between peers or in a workplace due to the with resistance. In general, transforming existing prac-

introduction of information systems. ese “lateral pri- tices might be a precondition for engaging engineers

vacy” concerns are related to but o en distinct from con- who feel that privacy is an abstract problem, not an

cerns regarding organizations collecting and processing immediate problem, not their problem, or not a prob-

data, which privacy-by-policy approaches address, and lem at all.8

unintended inferences, which privacy by architecture In contrast, in privacy-by-architecture approaches,

tackles. e social computing perspective, in which infor- the conception and implementation of privacy-

mation systems facilitate social interactions, informs the protecting measures are mainly under the purview of

methods and techniques the approach uses. e objec- technical experts with in-depth knowledge of crypto-

tive is to design systems that respect social norms regard- graphic and tra c analysis techniques. e objective

ing information ows and address privacy in the context is to develop privacy tools or mechanisms that o er

of collective information practices.13,14 is approach’s formal guarantees that is, ful ll quanti able pri-

techniques can help design teams create interactions vacy properties such as anonymity. Developing PETs

respectful of social and ethical norms. Feedback mech- requires mastering sophisticated engineering skills

anisms about system functionality might help users mainly acquired through participation in the com-

evaluate the impact of system use on their privacy and munity of experts. E orts to integrate PETs into sys-

change their future behavior accordingly. In addition to tem engineers’ toolboxes or into larger systems have

a ending to individuals’ concerns, researchers evaluate been limited. e absence of methods to implement,

the potential impact of complex information systems on integrate, and maintain PETs and the scant a ention

groups of users and society in general. For instance, this given to socialization of the tools might pose obstacles

42

IEEE Security & Privacy

March/April 2016

to taking them from the lab into the wild. Even when experts integrate PETs into systems, they can face backlash,16 especially if the proposed mechanisms introduce usability or performance tradeoffs or meet political resistance.4
Vulnerabilities might arise as a result of treating the different approaches as if they’re solutions that can be applied independently of one another. For instance, whether a social network service’s photo-tagging feature is more acceptable when tags are made public before or after the data subject’s confirmation, and whether these tags should be revocable varies depending on each community’s data-sharing practices. Because their focus is on tag design, UX engineers might treat photo storage and accessibility to the service provider as irrelevant to their task. However, such separation of concerns assumes that those potential risks that arise due to the underlying system architecture are independent of the local tagging practices. As a consequence, users might feel empowered in negotiating their privacy in social settings, while becoming increasingly vulnerable to violations of privacy by powerful service providers. Similarly, system engineers might constrain information flows in a way that strongly complicates and limits userfacing design. Especially in the context of PETs, such matters could lead to usability problems that dampen system adoption.
Finally, even if all three approaches are applied in concert, some privacy concerns might fall out of scope. Illustrative of such shortcomings is the tendency of all three approaches to produce solutions that scale only to a single organization—a model that doesn’t reflect the way new services, such as software as a service, are provisioned or the way free software projects are organized. Similarly, the Internet and mobile communication networks are examples of global infrastructure that require a different lens. Design decisions applied to infrastructures can have grave implications for privacy protections that can be applied to technologies built on them. Since the Snowden revelations, efforts to apply privacy-by-architecture methods and techniques in digital infrastructure design have gained in prominence, for instance, considering data minimization to protect against TLS client fingerprinting. These efforts have shown that addressing privacy in the Internet’s underlying protocols, Web browsers, or GSM standards is slow, complex, and readily dominated by those with the greatest resources to influence the process. Such processes can be stalled easily if, for example, the parties paying the tradeoff costs for privacy protection aren’t reaping the benefits. Although engineering methodologies can’t solve these political conflicts, they might help improve the process of developing inclusive and effective privacy solutions for global infrastructures.

Building Blocks
In defining the field of privacy engineering, we lean on software engineering, the subfield of computer science concerned with all aspects of the production of information systems, including the conceptualization, design, maintenance, and removal from service. Owing to the complexity of privacy as a social and legal concept, we also borrow knowledge and know-how from privacy research and practice.
Responding to the methodological shortcomings we described, we follow Sjaak Brinkkemper’s lead on method engineering and define privacy engineering as the field of research and practice that designs, implements, adapts, and evaluates theories, methods, techniques, and tools to systematically capture and address privacy issues when developing sociotechnical systems.17 In this context,
■■ privacy-engineering methods are approaches for systematically capturing and addressing privacy issues during information system development, management, and maintenance;
■■ privacy-engineering techniques are procedures, possibly with a prescribed language or notation, to accomplish privacy-engineering tasks or activities; and
■■ privacy-engineering tools are (automated) means that support privacy engineers during part of a privacyengineering process.
The definition would benefit from some further elaboration. First of all, what justifies identifying an engineering activity as pertaining to privacy? And, how can we answer this question if we don’t settle on a definition of privacy? As Deirdre Mulligan expressed elegantly in her IWPE keynote, privacy-engineering work requires embracing the plurality, contextuality, and contestability of privacy as a social, political, and legal concept.
A primary example illustrating privacy’s plurality is the work of Daniel Solove, who distinguishes between the right to be left alone, limited access, control, personhood, secrecy, and intimacy based on an extensive study of torts in the US legal system.18
Contextuality is best described using the justificatory framework developed by Helen Nissenbaum, who argues that privacy isn’t about control over or confidentiality of information, but rather ensuring appropriate information flows respectful of social norms in a given context.13 For example, during a consultation, it’s appropriate for a patient to disclose health information to the doctor, but not vice versa.
Contestability refers to the availability of multiple concepts around which disputes exist that can’t be settled by an appeal to “empirical evidence, linguistic usage, or the canons of logic alone.”19 Contestability provides a

www.computer.org/security

43

IEEE SYMPOSIUM ON SECURITY AND PRIVACY

language for conversing about privacy’s meaning, allowing it to be flexible enough to capture very different privacy issues in rapidly changing sociotechnical systems (plurality) introduced in different contexts with varying information norms (contextuality).
To preserve its contestability, we refrain from folding a specific conception of privacy into the definition of the field. However, we do assume that any defendable privacy methodology will draw on some normative theory of privacy, be it legal, social, or political. In the absence of such a normative compass, we lack the parameters against which to judge whether a method, technique, or tool attends to privacy. For example, at IWPE, Guy Zyskind and his colleagues illustrated how the blockchain can combine with storage to provide a data management platform that equips users with greater control and transparency over their personal information.20 Although the blockchain can be used to fulfill very different goals, the authors evaluate its potential as a privacy-engineering technique. The selection, conceptualization, and appropriateness of privacy definitions for a privacy-engineering task are topics of substantial interest in the field.
In addition to exploring relevant theories of privacy and engineering, the field calls for the development of methods, techniques, and tools. At IWPE, Nicolas Notario and his colleagues presented and evaluated PRIPARE, a method to integrate existing privacyengineering best practices—including privacy requirements elicitation and architectural techniques—into the design process.21 Marit Hansen and her colleagues discussed a technique that would help engineers reconcile tensions between potentially conflicting privacy goals, like unlinkability and transparency.22 And finally, Fateme Shirazi and her colleagues compared experimental techniques and tools that engineers can use to assess the performance of or attacks on the Tor network without violating user privacy.23
Privacy engineering foresees the use of these methods, techniques, and tools in developing information systems. By information systems, we refer to not only the technical artifact but the greater sociotechnical system. In using this term, we recognize that any system exists only in interplay with a host of social, political, legal, and economic arrangements.1 We argue that those in the privacy-engineering field need to be cognizant of the greater material and social networks that the engineered artifacts exist in. They should also support sociotechnical design practices that aspire to develop efficient and effective approaches to privacy and that, in the process, help improve the lives of those affected by these systems.
Three papers at IWPE beautifully teased out the sociotechnical aspects of addressing privacy in infrastructures. Nick Doty, after providing an overview of

the methods followed by the Internet Engineering Task Force and the World Wide Web Consortium, described some tools to incentivize and evaluate the way privacy issues are addressed during the standards-­making process.24 Gina Fisk and her colleagues presented a method to minimize privacy risks in cybersecurity data sharing that prompted a discussion on the appropriateness of making privacy claims in infrastructures built for national security and surveillance.25 Eve Maler introduced an Internet-scalable consent mechanism that might prove to be the “sweet spot” that attends to both technical and regulatory challenges in the context of Internet of Things.26 In addition, IWPE’s second keynote speaker, Ian Oliver from Nokia Networks, illustrated how privacy engineers can leverage concepts and techniques from the safety-critical domain. He highlighted checklists, dataflow modeling, and organizational roles as aids to enabling good engineering culture.
The field’s robustness depends as much on the development of methodologies as it does on their implementation, adaptation, and evaluation. Reports and case studies that expose the challenges of implementing privacy technologies are elemental to the generalization and systematization of privacy-engineering knowledge. In their IWPE paper on secure two-party computation, Henrik Ziegeldorf and his colleagues implemented and evaluated the performance of different protocols to help nonexpert developers pick the framework that fits their needs.27 In the process, the authors documented implementation challenges unique to each protocol. Rainer Hörbe and Walter Hötzendorfer developed an evaluation technique for federated identity management systems that also can aid engineers in translating normative privacy principles into architectures.28
In addition to addressing gaps in research, our definition of privacy engineering is comprehensive enough to encapsulate recent efforts in developing standardized processes. Ann Cavoukian and her colleagues defined privacy as a nonfunctional requirement in the engineering process,11 whereas MITRE and the National Institute of Science and Technology characterized privacy engineering as a form of risk analysis.29,30 These definitions frame privacy narrowly and constrain the type of methodologies that can be used to those that are risk based. They’re skewed toward privacy-by-policy approaches and barely attend to privacy-by-interaction methodologies. Informed by the diversity of research and practice, our definition of privacy engineering provides a broader framework in which existing and future efforts can be cultivated.
Looking Ahead
Efforts to address privacy using technical means are still

44

IEEE Security & Privacy

March/April 2016

scattered and disconnected. Few of these efforts explicitly attend to generalizing and systematizing associated engineering practices so as to be accessible to a wider community. Public and private organizations’ continuing negative track record of privacy blunders suggests both would benefit from the development of a privacyengineering practice.
Privacy engineering responds to these gaps, and IWPE is a forum where community members can come together to actively engage in the nascence of this new field. At its first successful iteration, IWPE participants responded to the field’s challenges, identified gaps, and agreed on three issues that require urgent attention.
First, we need to develop methodologies to address concerns of parties’ increased capacity to use machine learning to draw inferences from datasets. With advances in software as a service, big data infrastructures, and artificial intelligence, greater inferences can be made about individuals and user populations. These inferences can be used to profile users, organize future interactions, and drive a shift to data-centric software engineering practice. What methods, techniques, and tools address surveillance, discrimination, and accountability concerns attributed to such semantic power in sociotechnical systems?
Second, we must conduct empirical studies that reflect on different contextual challenges to applying privacy-engineering methods, techniques, and tools. Implicit assumptions about system architectures, labor, expertise, and organization type underlie methods, techniques, and tools. Empirical studies that explore how privacy issues are (or aren’t) currently addressed in different engineering contexts and that evaluate which methods, techniques, and tools are more appropriate in a given context are crucial to the field’s success.
Finally, we need metrics and analytics to evaluate the efficacy of privacy-engineering activities. Metrics can be used to indicate the number of privacy violations, track the number of a system’s fulfilled privacy requirements, choose privacy tools, or evaluate privacy and performance tradeoffs. In some cases, rather than quantification, analytical evaluation based on interdisciplinary methodologies might be more appropriate. Both approaches are hot topics of future research.
T hese are a subset of the exciting challenges at the core of privacy engineering. We welcome the growing community of privacy-engineering research and practice to join us in further shaping this field at the next IWPE, to be held 25–26 May 2016, in San Jose, California, co-located with the 37th IEEE Symposium on Security and Privacy. Further information on IWPE

2016 can be found at http://ieee-security.org/TC /SPW2016/IWPE.
Acknowledgments We thank our Organizing and Program Committee, the authors and attendees who contributed to the fruitful discussions held during the workshop sessions, and the IEEE Symposium on Security and Privacy workshop organizers for giving us the chance to kick off the First International Workshop on Privacy Engineering. We’re especially indebted to Helen Nissenbaum, Carmela Troncoso, Jaap-Henk ­Hoepman, and Yod-Samuel Martin as well as the anonymous reviewers of this article for their comments. This work was completed with the generous support of the Information Law Institute at New York University, the Center for Information Technology Policy at Princeton University, a grant from the Flemish Research Council, and the PRIPARE project funded by the EU’s 7th Framework Programme under grant agreement ICT-610613.
References 1. K.A. Bamberger and D.K. Mulligan, “New Governance,
Chief Privacy Officers, and the Corporate Management of Information Privacy in the United States: An Initial Inquiry,” Law Policy, vol. 33, no. 4, 2011, pp. 477–508. 2. “Chronology of Data Breaches: Security Breaches 2005– Present,” Privacy Rights Clearinghouse, Apr. 2005; https://www.privacyrights.org/data-breach. 3. N.P.J. Larson and S. Shane, “N.S.A. Able to Foil Basic Safeguards of Privacy on Web,” New York Times, 5 Sept. 2013; w w w. ny t i m e s .c o m / 2 0 1 3 / 0 9 / 0 6 / u s / n s a - f o i l s - m u c h -internet-encryption.html. 4. G. Danezis and S. Gürses, “A Critical Review of 10 Years of Privacy Technology,” Proc. Surveillance Cultures: A Global Surveillance Society, 2010; http://homes .esat.kuleuven.be/~sguerses/papers/DanezisGuerses SurveillancePets2010.pdf. 5. A. Narayanan and V. Shmatikov, “Robust De-­ anonymization of Large Sparse Datasets,” Proc. IEEE Symp. Security and Privacy, 2008, pp. 111–125. 6. G. Acar et al., “FPDetective: Dusting the Web for Fingerprinters,” Proc. ACM SIGSAC Conf. Computer & Communications Security (CCS 13), 2013, pp. 1129–1140. 7. S. Gürses and C. Diaz, “Two Tales of Privacy in Online Social Networks,” IEEE Security & Privacy, vol. 11, no. 3, 2013, pp. 29–37. 8. S. Spiekermann and L.F. Cranor, “Engineering Privacy,” IEEE Trans. Software Eng., vol. 35, no. 1, 2009, pp. 67–82. 9. C. Kalloniatis, E. Kavakli, and S. Gritzalis, “Addressing Privacy Requirements in System Design: The PriS Method,” Requirements Eng., vol. 13, no. 3, 2008, pp. 241–255. 10. D.K. Mulligan and J. King, “Bridging the Gap between Privacy and Design,” Univ. Pennsylvania J. Constitutional Law, vol. 14, no. 4, 2011, p. 989.

www.computer.org/security

45

IEEE SYMPOSIUM ON SECURITY AND PRIVACY

11. A. Cavoukian, S. Shapiro, and R.J. Cronk, “Privacy Engineering: Proactively Embedding Privacy, by Design,” Information and Privacy Commissioner O ce, Government of Ontario, 2014; h ps://www.ipc.on.ca/images /Resources/pbd-priv-engineering.pdf.
12. “Proposal for a Regulation of the European Parliament and of the Council on the Protection of Individuals with Regard to the Processing of Personal Data and on the Free Movement of Such Data (General Data Protection Regulation),” COM/2012/011, European Commission, 2012.
13. H. Nissenbaum, Privacy in Context: Technology, Policy and the Integrity of Social Life,” Stanford Univ. Press, 2009.
14. L. Palen and P. Dourish, “Unpacking Privacy for a Networked World,” Proc. SIGCHI Conf. Human Factors in Computing Systems, 2003, pp. 129–136.
15. C. Dwork et al., “Fairness through Awareness,” Proc. 3rd Conf. Innovations in eoretical Computer Science, 2012, pp. 214–226.
16. R. Dingledine and N. Mathewson, “Anonymity Loves Company: Usability and the Network E ect,” Security and Usability: Designing Secure Systems that People Can Use, L. Cranor and S. Gar nkel, eds., 2005, pp. 547–559.
17. S. Brinkkemper, “Method Engineering: Engineering of Information Systems Development Methods and Tools,” Information and So ware Technology, vol. 38, no. 4, 1996, pp. 275–280.
18. D. Solove, “A Taxonomy of Privacy,” Univ. Pennsylvania Law Rev., vol. 154, no. 3, 2006, p. 477.
19. D.K. Mulligan and C. Koopman, “ eorizing Privacy’s Contestability: A Multi-dimensional Analytic of Privacy,” iConferences Proc. Special Workshop on Information Privacy, 2013, pp. 1026–1029.
20. G. Zyskind et al., “Decentralizing Privacy: Using Blockchain to Protect Personal Data,” Proc. IEEE Security and Privacy Workshops, 2015, pp. 180–184.
21. N. Notario et al., “PRIPARE: Integrating Privacy Best Practices into a Privacy Engineering Methodology,” Proc. IEEE Security and Privacy Workshops, 2015, pp. 151–158.
22. M. Hansen et al., “Protection Goals for Privacy Engineering,” Proc. IEEE Security and Privacy Workshops, 2015, pp. 159–166.
23. F. Shirazi et al., “Tor Experimentation Tools,” Proc. IEEE Security and Privacy Workshops, 2015, pp. 206–213.

24. N. Doty, “Reviewing for Privacy in Internet and Web Standard-Se ing,” Proc. IEEE Security and Privacy Workshops, 2015, pp. 185–192
25. G. Fisk et al., “Privacy Principles for Sharing Cyber Security Data,” Proc. IEEE Security and Privacy Workshops, 2015, pp. 193–197.
26. E. Maler, “Extending the Power of Consent with UserManaged Access: A Standard Architecture for Asynchronous, Centralizable, Internet-Scalable Consent,” Proc. IEEE Security and Privacy Workshops, 2015, pp. 175–179.
27. J.H. Ziegeldor et al., “Choose Wisely: A Comparison of Secure Two-Party Computation Frameworks,” Proc. IEEE Security and Privacy Workshops, 2015, pp. 198–205.
28. R. Hörbe and W. Hötzendorfer, “Privacy by Design in Federated Identity Management,” Proc. IEEE Security and Privacy Workshops, 2015, pp. 167–174.
29. S. Shapiro et al., “Privacy Engineering Framework,” MITRE, Aug. 2014; www.mitre.org/publications /technical-papers/privacy-engineering-framework.
30. “NISTIR 8062: Privacy Risk Management for Federal Information Systems,” S. Brooks and E. Nadeau, eds., Nat’l Inst. Standards and Technology, May 2015; http://csrc.nist.gov/publications/drafts/nistir-8062 /nistir_8062_dra .pdf.
Seda Gürses is a postdoctoral research associate at Princeton University’s Center for Information Technology Policy and an FWO (Fonds Wetenschappelijk Onderzoek–Vlaanderen) fellow at COSIC, University of Leuven. She works on privacy and requirements engineering, privacy-enhancing technologies, and surveillance. Gürses received a PhD in computer science at the University of Leuven, Belgium. Contact her at fgurses@princeton.edu.
Jose M. del Alamo is an associate professor in the Information and Communications Technology (ICT) Systems Engineering Department at the Universidad Politécnica de Madrid. His research focuses on personal data management issues, including privacy and identity management, in the context of so ware and systems engineering. Del Alamo received a PhD in ICT systems engineering from the Universidad Politécnica de Madrid. Contact him at jm.delalamo@upm.es.

Subscribe today for the latest in computational science and engineering research, news and analysis, CSE in education, and emerging technologies in the hard sciences.
www.computer.org/cise

46

IEEE Security & Privacy

March/April 2016

